import os
import time

# 1. è®¾ç½® HuggingFace é•œåƒï¼Œé˜²æ­¢ä¸‹è½½æ¨¡å‹è¶…æ—¶
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# --- å…³é”®ä¿®æ”¹ç‚¹ï¼šæ›´æ–°äº†å¯¼å…¥è·¯å¾„ä»¥åŒ¹é…æœ€æ–°ç‰ˆ LangChain ---
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter  # æ–°çš„é—¨ç‰Œå·
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS


def create_vector_db():
    print("ğŸš€ å¼€å§‹æ„å»ºçŸ¥è¯†åº“...")

    # --- ç¬¬ä¸€æ­¥ï¼šåŠ è½½å¤šæœ¬ä¹¦ ---
    file_paths = ["data/textbook.txt", "data/workbook.txt"]
    all_documents = []

    for file_path in file_paths:
        if os.path.exists(file_path):
            print(f"ğŸ“– æ­£åœ¨è¯»å–: {file_path} ...")
            try:
                loader = TextLoader(file_path, encoding="gb18030")
                docs = loader.load()
                all_documents.extend(docs)
            except Exception as e:
                print(f"âŒ è¯»å–é”™è¯¯ {file_path}: {e}")
        else:
            print(f"âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ°æ–‡ä»¶ {file_path}ï¼Œè·³è¿‡ã€‚")

    if not all_documents:
        print("âŒ æ²¡æœ‰è¯»å–åˆ°ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥ data æ–‡ä»¶å¤¹ï¼")
        return

    print(f"1. æ•°æ®åŠ è½½å®Œæ¯•ï¼Œå…±è¯»å– {len(all_documents)} ä¸ªæ–‡æ¡£å¯¹è±¡")

    # --- ç¬¬äºŒæ­¥ï¼šæ–‡æœ¬åˆ‡ç‰‡ (Chunking) ---
    print("2. æ­£åœ¨è¿›è¡Œæ–‡æœ¬åˆ‡ç‰‡...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_documents(all_documents)
    print(f"âœ… åˆ‡ç‰‡å®Œæ¯•ï¼åŸä¹¦åˆè®¡è¢«åˆ‡åˆ†ä¸º {len(texts)} ä¸ªçŸ¥è¯†ç‰‡æ®µ")

    # --- ç¬¬ä¸‰æ­¥ï¼šåŠ è½½åµŒå…¥æ¨¡å‹ (Embedding) ---
    print("3. æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹ (shibing624/text2vec-base-chinese)...")
    print("   (é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½çº¦ 400MB æ¨¡å‹æ–‡ä»¶ï¼Œè¯·è€å¿ƒç­‰å¾…...)")
    # ä½¿ç”¨å›½å†…é•œåƒæºä¸‹è½½æ¨¡å‹
    embeddings = HuggingFaceEmbeddings(
        model_name="shibing624/text2vec-base-chinese",
        model_kwargs={'device': 'cpu'}  # å»ºåº“ç”¨CPUè¶³å¤Ÿå¿«ï¼Œä¸”æœ€ç¨³å®š
    )

    # --- ç¬¬å››æ­¥ï¼šå‘é‡åŒ–å¹¶å­˜å‚¨ (Indexing) ---
    print("4. æ­£åœ¨å°†çŸ¥è¯†ç‰‡æ®µè½¬åŒ–ä¸ºå‘é‡...")
    db = FAISS.from_documents(texts, embeddings)

    # ä¿å­˜åˆ°æœ¬åœ°
    save_path = "data/control_knowledge_base"
    db.save_local(save_path)
    print(f"ğŸ‰ æˆåŠŸï¼çŸ¥è¯†åº“å·²æ„å»ºå®Œæˆï¼Œä¿å­˜åœ¨: {save_path}")


if __name__ == "__main__":
    create_vector_db()