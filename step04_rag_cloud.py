import os
import time
import torch
from modelscope import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

# --- 1. è·¯å¾„é€‚é… (å…³é”®ä¿®æ”¹) ---
# åœ¨ Kaggle è¿è¡Œæ—¶ï¼Œä½¿ç”¨äº‘ç«¯è·¯å¾„ï¼›åœ¨æœ¬åœ°å¤‡ä»½æ—¶ï¼Œä½¿ç”¨æœ¬åœ°è·¯å¾„
if os.path.exists("/kaggle/input"):
    DATA_PATH = "/kaggle/input/nuaa-control-qa/control_knowledge_base"
    DEVICE = "cuda"  # äº‘ç«¯ä½¿ç”¨ GPU
else:
    # è¿™é‡Œçš„è·¯å¾„æ”¹ä¸ºä½ æœ¬åœ°ç”µè„‘çš„å®é™…è·¯å¾„
    DATA_PATH = "./data/control_knowledge_base"
    DEVICE = "cpu"  # æœ¬åœ°å› ä¸ºé©±åŠ¨é—®é¢˜æš‚æ—¶ç”¨ CPU

print(f"ğŸ§  æ­£åœ¨åŠ è½½æ¨¡å‹ä¸å‘é‡åº“ (è¿è¡Œè®¾å¤‡: {DEVICE})...")

# --- 2. æ ¸å¿ƒé€»è¾‘ ---
# åŠ è½½ Embedding æ¨¡å‹
embeddings = HuggingFaceEmbeddings(model_name="shibing624/text2vec-base-chinese")

# åŠ è½½å‘é‡åº“
# allow_dangerous_deserialization=True æ˜¯å› ä¸ºæœ¬åœ°è¯»å–è‡ªå·±ç”Ÿæˆçš„ pkl æ–‡ä»¶æ˜¯å®‰å…¨çš„
vector_db = FAISS.load_local(DATA_PATH, embeddings, allow_dangerous_deserialization=True)

# åŠ è½½ Qwen2.5-1.5B æ¨¡å‹
model_dir = snapshot_download("Qwen/Qwen2.5-1.5B-Instruct")
tokenizer = AutoTokenizer.from_pretrained(model_dir)

# æ ¹æ®è®¾å¤‡åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    model_dir,
    device_map=DEVICE,
    torch_dtype="auto"
)


def rag_chat(query):
    # æ£€ç´¢
    docs = vector_db.similarity_search(query, k=3)
    context = "\n".join([doc.page_content for doc in docs])

    # æ„é€  Prompt
    prompt = f"ä½ æ˜¯ä¸€ä¸ªå—èˆªè‡ªåŠ¨åŒ–å­¦é™¢çš„åŠ©æ•™ã€‚è¯·æ ¹æ®ä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ã€‚\nå‚è€ƒèµ„æ–™ï¼š\n{context}\né—®é¢˜ï¼š{query}"
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([text], return_tensors="pt").to(DEVICE)

    # ç”Ÿæˆå›ç­”
    generated_ids = model.generate(**model_inputs, max_new_tokens=512)
    response = tokenizer.batch_decode(generated_ids[:, model_inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]
    return response


if __name__ == "__main__":
    question = "ä»€ä¹ˆæ˜¯è‡ªåŠ¨æ§åˆ¶ç³»ç»Ÿçš„ç¨³æ€è¯¯å·®ï¼Ÿ"
    print(f"é—®é¢˜: {question}")
    # æ³¨æ„ï¼šæœ¬åœ° CPU è·‘è¿™æ®µä¼šæ¯”è¾ƒæ…¢ï¼Œå¤§çº¦éœ€è¦ 1 åˆ†é’Ÿ
    answer = rag_chat(question)
    print(f"å›ç­”: {answer}")